Today i added the processing workers implemetntation and test bed.  Working thru just getting the 
first test function fixed, i discovereed all these error/typos:

```
bash
PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 0 items                                                                                                                            

=========================================================== no tests ran in 0.00s ============================================================
ERROR: file or directory not found: tests/pipeline/test_processing_workers.py

(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py^C
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem/tests/pipeline/test_processing_workers..py^C
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers..py
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 0 items / 1 error                                                                                                                  

=================================================================== ERRORS ===================================================================
________________________________________ ERROR collecting tests/pipeline/test_processing_workers..py _________________________________________
ImportError while importing test module '/home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem/tests/pipeline/test_processing_workers..py'.
Hint: make sure your test modules/packages have valid Python names.
Traceback:
../../anaconda3/envs/multicloudtest/lib/python3.9/importlib/__init__.py:127: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
E   ModuleNotFoundError: No module named 'tests.pipeline.test_processing_workers'
========================================================== short test summary info ===========================================================
ERROR tests/pipeline/test_processing_workers..py
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
============================================================== 1 error in 0.11s ==============================================================
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ ls
config  deployment  docs  htmlcov  README.md  received_chunks  requirements.txt  src  tests
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ cd tests
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem/tests$ ls
auth  communication  config  coordination  __init__.py  pipeline  __pycache__  run_all_tests.sh
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem/tests$ cd pipeline
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem/tests/pipeline$ ls
__init__.py  __pycache__  test_ingestion_engine.py  test_processing_workers..py
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem/tests/pipeline$ mv test_processing_workers..py test_processing_workers.py
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem/tests/pipeline$ cd ../../
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers..py
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 0 items                                                                                                                            

=========================================================== no tests ran in 0.00s ============================================================
ERROR: file or directory not found: tests/pipeline/test_processing_workers..py

(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 0 items                                                                                                                            

=========================================================== no tests ran in 0.01s 

```
All that junk errors above were related to 1) a double '..' on the file name. it ws named <name>..py instead of <name>.py and then when i mv'd it,-> i dont knwo the file was empty.  so i had to put the tests back in.  

Now below I have commmented out all but the first test and mock settings.  Lets see what we find:

```
bash
============================================================
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 0 items / 1 error                                                                                                                  

=================================================================== ERRORS ===================================================================
_________________________________________ ERROR collecting tests/pipeline/test_processing_workers.py _________________________________________
../../anaconda3/envs/multicloudtest/lib/python3.9/site-packages/_pytest/python.py:498: in importtestmodule
    mod = import_path(
../../anaconda3/envs/multicloudtest/lib/python3.9/site-packages/_pytest/pathlib.py:587: in import_path
    importlib.import_module(module_name)
../../anaconda3/envs/multicloudtest/lib/python3.9/importlib/__init__.py:127: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
<frozen importlib._bootstrap>:1030: in _gcd_import
    ???
<frozen importlib._bootstrap>:1007: in _find_and_load
    ???
<frozen importlib._bootstrap>:986: in _find_and_load_unlocked
    ???
<frozen importlib._bootstrap>:680: in _load_unlocked
    ???
../../anaconda3/envs/multicloudtest/lib/python3.9/site-packages/_pytest/assertion/rewrite.py:186: in exec_module
    exec(co, module.__dict__)
tests/pipeline/test_processing_workers.py:6: in <module>
    from src.pipeline.processing_workers import (
E     File "/home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem/src/pipeline/processing_workers.py", line 104
E       processing_config=self.config.get('processing' {})
E                                                      ^
E   SyntaxError: invalid syntax
========================================================== short test summary info ===========================================================
ERROR tests/pipeline/test_processing_workers.py
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
============================================================== 1 error in 0.16s ==============================================================
```

I forgot a comma above.

```
bash

(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 0 items / 1 error                                                                                                                  

=================================================================== ERRORS ===================================================================
_________________________________________ ERROR collecting tests/pipeline/test_processing_workers.py _________________________________________
tests/pipeline/test_processing_workers.py:6: in <module>
    from src.pipeline.processing_workers import (
src/pipeline/processing_workers.py:18: in <module>
    @dataclasss
E   NameError: name 'dataclasss' is not defined
========================================================== short test summary info ===========================================================
ERROR tests/pipeline/test_processing_workers.py - NameError: name 'dataclasss' is not defined
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
============================================================== 1 error in 0.16s ==============================================================


(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 0 items / 1 error                                                                                                                  


=================================================================== ERRORS ===================================================================
_________________________________________ ERROR collecting tests/pipeline/test_processing_workers.py _________________________________________
tests/pipeline/test_processing_workers.py:7: in <module>
    from src.pipeline.processing_workers import (
src/pipeline/processing_workers.py:18: in <module>
    @dataclasss
E   NameError: name 'dataclasss' is not defined
========================================================== short test summary info ===========================================================
ERROR tests/pipeline/test_processing_workers.py - NameError: name 'dataclasss' is not defined
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
============================================================== 1 error in 0.16s ==============================================================

```
Above we have the classic extra s in dataClasss import error.

```
bash
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 0 items / 1 error                                                                                                                  

=================================================================== ERRORS ===================================================================
_________________________________________ ERROR collecting tests/pipeline/test_processing_workers.py _________________________________________
tests/pipeline/test_processing_workers.py:7: in <module>
    from src.pipeline.processing_workers import (
src/pipeline/processing_workers.py:67: in <module>
    class DataTransformer(ProcessingFunction):
E   NameError: name 'ProcessingFunction' is not defined
========================================================== short test summary info ===========================================================
ERROR tests/pipeline/test_processing_workers.py - NameError: name 'ProcessingFunction' is not defined
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
============================================================== 1 error in 0.16s ==============================================================
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py

```

Another classci typo...i cant actually see the error, but i pasted ProcessingFunction from another line (function signatre) that didn't have any errors.

```
bash
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 1 item                                                                                                                             

tests/pipeline/test_processing_workers.py F                                                                                            [100%]

================================================================== FAILURES ==================================================================
______________________________________________________ test_worker_pool_initialization _______________________________________________________

mock_node_registry = namespace(nodes={'aws-node-1': namespace(node_id='aws-node-1', cloud_provider='aws', status='healthy'), 'gcp-node-1': ...vider='gcp', status='healthy'), 'gcp-node-2': namespace(node_id='gcp-node-2', cloud_provider='gcp', status='healthy')})

    @pytest.mark.asyncio
    async def test_worker_pool_initialization(mock_node_registry):
        """Test worker pool initializes correctly"""
>       worker_pool = ProcessingWorkerPool(mock_node_registry)

tests/pipeline/test_processing_workers.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/pipeline/processing_workers.py:12bove we find the classic 'extra s' error in dataclasss, so fixed that
2: in __init__
    self.processing_pipeline = self._initialize_processing_pipeline()
src/pipeline/processing_workers.py:156: in _initialize_processing_pipeline
    pipeline.append(DataValidator(step_name, step_config))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.pipeline.processing_workers.DataValidator object at 0x736b3dbfef70>, name = 'validate_data'
config = {'enabled': True, 'name': 'validate_data', 'timeout_seconds': 30}

    def __init__(self, name: str, config:Dict):
>       self.name-name
E       AttributeError: 'DataValidator' object has no attribute 'name'

src/pipeline/processing_workers.py:48: AttributeError
========================================================== short test summary info ===========================================================
FAILED tests/pipeline/test_processing_workers.py::test_worker_pool_initialization - AttributeError: 'DataValidator' object has no attribute 'name'
============================================================= 1 failed in 0.09s ==============================================================
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py

```
The above had '-' instead of '=' in that line. So used a dash for an assignmetn op. Not gonna work

```
bash
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 1 item                                                                                                                             

tests/pipeline/test_processing_workers.py .                                                                                            [100%]

============================================================= 1 passed in 0.03s ==============================================================

```
First test success. onto the next






Here we just found a whole heap of typos...i will highlight them:

```
bash
PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py


    @pytest.mark.asyncio
    async def test_process_chunks_success(mock_node_registry, mock_chunks):
        """Test successful processing of chunks"""
        worker_pool = ProcessingWorkerPool(mock_node_registry)
    
>       results = await worker_pool.process_chunks(mock_chunks)

tests/pipeline/test_processing_workers.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/pipeline/processing_workers.py:185: in process_chunks
    self._initailize_node_workloads()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.pipeline.processing_workers.ProcessingWorkerPool object at 0x77225863a790>

    def _initailize_node_workloads(self):
        """initialzie workload tracking for all of the nodes"""
        for node_id, node_info, in self.node_registry.nodes.items():
            if node_info.status=='healthy':
>               self.node.worklodas[node_id]=NodeWorkload(
                    node_id=node_id,
                    cloud_provider=node_info.cloud_provider
                )
E               AttributeError: 'ProcessingWorkerPool' object has no attribute 'node'

src/pipeline/processing_workers.py:171: AttributeError
------------------------------------------------------------ Captured stdout call ------------------------------------------------------------
⚡ Processing Worker Pool initialized
   Max workers per node: 4
   Load balancing: least_loaded
   Processing pipeline: 2 steps
   Simulation mode: True

⚡ Starting distributed processing of 10 chunks...
========================================================== short test summary info ===========================================================
FAILED tests/pipeline/test_processing_workers.py::test_process_chunks_success - AttributeError: 'ProcessingWorkerPool' object has no attribute 'node'
======================================================== 1 failed, 1 passed in 0.10s =========================================================
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 2 items                                                                                                                            

tests/pipeline/test_processing_workers.py .F                                                                                           [100%]

================================================================== FAILURES ==================================================================
________________________________________________________ test_process_chunks_success _________________________________________________________

mock_node_registry = namespace(nodes={'aws-node-1': namespace(node_id='aws-node-1', cloud_provider='aws', status='healthy'), 'gcp-node-1': ...vider='gcp', status='healthy'), 'gcp-node-2': namespace(node_id='gcp-node-2', cloud_provider='gcp', status='healthy')})
mock_chunks = [namespace(chunk_id='test_chunk_0', data=b'test data content 0test data content 0test data content 0test data content ...t data content 5test data content 5test data content 5test data content 5test data content 5test data content 5'), ...]

    @pytest.mark.asyncio
    async def test_process_chunks_success(mock_node_registry, mock_chunks):
        """Test successful processing of chunks"""
        worker_pool = ProcessingWorkerPool(mock_node_registry)
    
>       results = await worker_pool.process_chunks(mock_chunks)

tests/pipeline/test_processing_workers.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/pipeline/processing_workers.py:185: in process_chunks
    self._initailize_node_workloads()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.pipeline.processing_workers.ProcessingWorkerPool object at 0x7eb49a66d970>

    def _initailize_node_workloads(self):
        """initialzie workload tracking for all of the nodes"""
        for node_id, node_info, in self.node_registry.nodes.items():
            if node_info.status=='healthy':
>               self.node.worklodas[node_id]=NodeWorkload(
                    node_id=node_id,
                    cloud_provider=node_info.cloud_provider
                )
E               AttributeError: 'ProcessingWorkerPool' object has no attribute 'node'

src/pipeline/processing_workers.py:171: AttributeError
------------------------------------------------------------ Captured stdout call ------------------------------------------------------------
⚡ Processing Worker Pool initialized
   Max workers per node: 4
   Load balancing: least_loaded
   Processing pipeline: 2 steps
   Simulation mode: True

⚡ Starting distributed processing of 10 chunks...
========================================================== short test summary info ===========================================================
FAILED tests/pipeline/test_processing_workers.py::test_process_chunks_success - AttributeError: 'ProcessingWorkerPool' object has no attribute 'node'
======================================================== 1 failed, 1 passed in 0.09s =========================================================


```
node.workloads ::-> change to correct: node_workloads

```
bash
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 2 items                                                                                                                            

tests/pipeline/test_processing_workers.py .F                                                                                           [100%]

================================================================== FAILURES ==================================================================
________________________________________________________ test_process_chunks_success _________________________________________________________

mock_node_registry = namespace(nodes={'aws-node-1': namespace(node_id='aws-node-1', cloud_provider='aws', status='healthy'), 'gcp-node-1': ...vider='gcp', status='healthy'), 'gcp-node-2': namespace(node_id='gcp-node-2', cloud_provider='gcp', status='healthy')})
mock_chunks = [namespace(chunk_id='test_chunk_0', data=b'test data content 0test data content 0test data content 0test data content ...t data content 5test data content 5test data content 5test data content 5test data content 5test data content 5'), ...]

    @pytest.mark.asyncio
    async def test_process_chunks_success(mock_node_registry, mock_chunks):
        """Test successful processing of chunks"""
        worker_pool = ProcessingWorkerPool(mock_node_registry)
    
>       results = await worker_pool.process_chunks(mock_chunks)

tests/pipeline/test_processing_workers.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.pipeline.processing_workers.ProcessingWorkerPool object at 0x703756e42400>
chunks = [namespace(chunk_id='test_chunk_0', data=b'test data content 0test data content 0test data content 0test data content ...t data content 5test data content 5test data content 5test data content 5test data content 5test data content 5'), ...]

    async def process_chunks(self, chunks: List) -> List[ProceessingFunction]:
        """main entry: here is whrere we will process all chunks
        across all the available nodes
        Args: chunks: list of datachunk objects from ingestion engine
        Returns:List of ProcessingTask results"""
    
        print(f"\n⚡ Starting distributed processing of {len(chunks)} chunks...")
        #initialisze node work load tracking
>       self._initailize_node_workloads()
E       AttributeError: 'ProcessingWorkerPool' object has no attribute '_initailize_node_workloads'

src/pipeline/processing_workers.py:185: AttributeError
------------------------------------------------------------ Captured stdout call ------------------------------------------------------------
⚡ Processing Worker Pool initialized
   Max workers per node: 4
   Load balancing: least_loaded
   Processing pipeline: 2 steps
   Simulation mode: True

⚡ Starting distributed processing of 10 chunks...
========================================================== short test summary info ===========================================================
FAILED tests/pipeline/test_processing_workers.py::test_process_chunks_success - AttributeError: 'ProcessingWorkerPool' object has no attribute '_initailize_node_workloads'
======================================================== 1 failed, 1 passed in 0.09s =========================================================
```
Initialize was spelled wrong

```
bash


(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 2 items                                                                                                                            

tests/pipeline/test_processing_workers.py .F                                                                                           [100%]

================================================================== FAILURES ==================================================================
________________________________________________________ test_process_chunks_success _________________________________________________________

mock_node_registry = namespace(nodes={'aws-node-1': namespace(node_id='aws-node-1', cloud_provider='aws', status='healthy'), 'gcp-node-1': ...vider='gcp', status='healthy'), 'gcp-node-2': namespace(node_id='gcp-node-2', cloud_provider='gcp', status='healthy')})
mock_chunks = [namespace(chunk_id='test_chunk_0', data=b'test data content 0test data content 0test data content 0test data content ...t data content 5test data content 5test data content 5test data content 5test data content 5test data content 5'), ...]

    @pytest.mark.asyncio
    async def test_process_chunks_success(mock_node_registry, mock_chunks):
        """Test successful processing of chunks"""
        worker_pool = ProcessingWorkerPool(mock_node_registry)
    
>       results = await worker_pool.process_chunks(mock_chunks)

tests/pipeline/test_processing_workers.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/pipeline/processing_workers.py:185: in process_chunks
    self._initialize_node_workloads()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.pipeline.processing_workers.ProcessingWorkerPool object at 0x7992fce79400>

    def _initialize_node_workloads(self):
        """initialzie workload tracking for all of the nodes"""
        for node_id, node_info, in self.node_registry.nodes.items():
            if node_info.status=='healthy':
>               self.node.workloads[node_id]=NodeWorkload(
                    node_id=node_id,
                    cloud_provider=node_info.cloud_provider
                )
E               AttributeError: 'ProcessingWorkerPool' object has no attribute 'node'

src/pipeline/processing_workers.py:171: AttributeError
------------------------------------------------------------ Captured stdout call ------------------------------------------------------------
⚡ Processing Worker Pool initialized
   Max workers per node: 4
   Load balancing: least_loaded
   Processing pipeline: 2 steps
   Simulation mode: True

⚡ Starting distributed processing of 10 chunks...
========================================================== short test summary info ===========================================================
```
same node.workload -> node_workload on a different line


```
bash

FAILED tests/pipeline/test_processing_workers.py::test_process_chunks_success - AttributeError: 'ProcessingWorkerPool' object has no attribute 'node'
======================================================== 1 failed, 1 passed in 0.10s =========================================================
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 2 items                                                                                                                            

tests/pipeline/test_processing_workers.py .F                                                                                           [100%]

================================================================== FAILURES ==================================================================
________________________________________________________ test_process_chunks_success _________________________________________________________

mock_node_registry = namespace(nodes={'aws-node-1': namespace(node_id='aws-node-1', cloud_provider='aws', status='healthy'), 'gcp-node-1': ...vider='gcp', status='healthy'), 'gcp-node-2': namespace(node_id='gcp-node-2', cloud_provider='gcp', status='healthy')})
mock_chunks = [namespace(chunk_id='test_chunk_0', data=b'test data content 0test data content 0test data content 0test data content ...t data content 5test data content 5test data content 5test data content 5test data content 5test data content 5'), ...]

    @pytest.mark.asyncio
    async def test_process_chunks_success(mock_node_registry, mock_chunks):
        """Test successful processing of chunks"""
        worker_pool = ProcessingWorkerPool(mock_node_registry)
    
>       results = await worker_pool.process_chunks(mock_chunks)

tests/pipeline/test_processing_workers.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/pipeline/processing_workers.py:198: in process_chunks
    await self._process_tasks_with_concurrency()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.pipeline.processing_workers.ProcessingWorkerPool object at 0x79b8c9f82f70>

    async def _process_tasks_with_concurrency(self):
        """Process takes within/at concurrencty limit"""
>       while self.pending_task or self.active_tasks:
E       AttributeError: 'ProcessingWorkerPool' object has no attribute 'pending_task'

src/pipeline/processing_workers.py:215: AttributeError
------------------------------------------------------------ Captured stdout call ------------------------------------------------------------
⚡ Processing Worker Pool initialized
   Max workers per node: 4
   Load balancing: least_loaded
   Processing pipeline: 2 steps
   Simulation mode: True

⚡ Starting distributed processing of 10 chunks...
========================================================== short test summary info ===========================================================
FAILED tests/pipeline/test_processing_workers.py::test_process_chunks_success - AttributeError: 'ProcessingWorkerPool' object has no attribute 'pending_task'
======================================================== 1 failed, 1 passed in 0.09s =========================================================



bash
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 2 items                                                                                                                            

tests/pipeline/test_processing_workers.py .F                                                                                           [100%]

================================================================== FAILURES ==================================================================
________________________________________________________ test_process_chunks_success _________________________________________________________

mock_node_registry = namespace(nodes={'aws-node-1': namespace(node_id='aws-node-1', cloud_provider='aws', status='healthy'), 'gcp-node-1': ...vider='gcp', status='healthy'), 'gcp-node-2': namespace(node_id='gcp-node-2', cloud_provider='gcp', status='healthy')})
mock_chunks = [namespace(chunk_id='test_chunk_0', data=b'test data content 0test data content 0test data content 0test data content ...t data content 5test data content 5test data content 5test data content 5test data content 5test data content 5'), ...]

    @pytest.mark.asyncio
    async def test_process_chunks_success(mock_node_registry, mock_chunks):
        """Test successful processing of chunks"""
        worker_pool = ProcessingWorkerPool(mock_node_registry)
    
>       results = await worker_pool.process_chunks(mock_chunks)

tests/pipeline/test_processing_workers.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/pipeline/processing_workers.py:198: in process_chunks
    await self._process_tasks_with_concurrency()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.pipeline.processing_workers.ProcessingWorkerPool object at 0x7ce1c9924c40>

    async def _process_tasks_with_concurrency(self):
        """Process takes within/at concurrencty limit"""
>       while self.pending_task or self.active_tasks:
E       AttributeError: 'ProcessingWorkerPool' object has no attribute 'pending_task'

src/pipeline/processing_workers.py:215: AttributeError
------------------------------------------------------------ Captured stdout call ------------------------------------------------------------
⚡ Processing Worker Pool initialized
   Max workers per node: 4
   Load balancing: least_loaded
   Processing pipeline: 2 steps
   Simulation mode: True

⚡ Starting distributed processing of 10 chunks...
========================================================== short test summary info ===========================================================
FAILED tests/pipeline/test_processing_workers.py::test_process_chunks_success - AttributeError: 'ProcessingWorkerPool' object has no attribute 'pending_task'
======================================================== 1 failed, 1 passed in 0.09s =========================================================
```
pending_task -> pending_tasks  (missed s)

``` 
bash 

(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 2 items                                                                                                                            

tests/pipeline/test_processing_workers.py .F                                                                                           [100%]

================================================================== FAILURES ==================================================================
________________________________________________________ test_process_chunks_success _________________________________________________________

mock_node_registry = namespace(nodes={'aws-node-1': namespace(node_id='aws-node-1', cloud_provider='aws', status='healthy'), 'gcp-node-1': ...vider='gcp', status='healthy'), 'gcp-node-2': namespace(node_id='gcp-node-2', cloud_provider='gcp', status='healthy')})
mock_chunks = [namespace(chunk_id='test_chunk_0', data=b'test data content 0test data content 0test data content 0test data content ...t data content 5test data content 5test data content 5test data content 5test data content 5test data content 5'), ...]

    @pytest.mark.asyncio
    async def test_process_chunks_success(mock_node_registry, mock_chunks):
        """Test successful processing of chunks"""
        worker_pool = ProcessingWorkerPool(mock_node_registry)
    
>       results = await worker_pool.process_chunks(mock_chunks)

tests/pipeline/test_processing_workers.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/pipeline/processing_workers.py:198: in process_chunks
    await self._process_tasks_with_concurrency()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.pipeline.processing_workers.ProcessingWorkerPool object at 0x7f5ee8e42d30>

    async def _process_tasks_with_concurrency(self):
        """Process takes within/at concurrencty limit"""
        while self.pending_tasks or self.active_tasks:
            #strt new tasks up to concurrency limit
            while (len(self.active_tasks) < self.max_concurrent_tasks and
                   self.pending_tasks):
    
                task = self.pending_tasks.pop(0)
    
                # Select node for this task
>               selected_node = self._select_node_for_task(task)
E               AttributeError: 'ProcessingWorkerPool' object has no attribute '_select_node_for_task'

src/pipeline/processing_workers.py:223: AttributeError
------------------------------------------------------------ Captured stdout call ------------------------------------------------------------
⚡ Processing Worker Pool initialized
   Max workers per node: 4
   Load balancing: least_loaded
   Processing pipeline: 2 steps
   Simulation mode: True

⚡ Starting distributed processing of 10 chunks...
========================================================== short test summary info ===========================================================
FAILED tests/pipeline/test_processing_workers.py::test_process_chunks_success - AttributeError: 'ProcessingWorkerPool' object has no attribute '_select_node_for_task'
======================================================== 1 failed, 1 passed in 0.10s =========================================================


```
put in "_" in front of fucntion call

```
bash
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 2 items                                                                                                                            

tests/pipeline/test_processing_workers.py .F                                                                                           [100%]

================================================================== FAILURES ==================================================================
________________________________________________________ test_process_chunks_success _________________________________________________________

mock_node_registry = namespace(nodes={'aws-node-1': namespace(node_id='aws-node-1', cloud_provider='aws', status='healthy'), 'gcp-node-1': ...vider='gcp', status='healthy'), 'gcp-node-2': namespace(node_id='gcp-node-2', cloud_provider='gcp', status='healthy')})
mock_chunks = [namespace(chunk_id='test_chunk_0', data=b'test data content 0test data content 0test data content 0test data content ...t data content 5test data content 5test data content 5test data content 5test data content 5test data content 5'), ...]

    @pytest.mark.asyncio
    async def test_process_chunks_success(mock_node_registry, mock_chunks):
        """Test successful processing of chunks"""
        worker_pool = ProcessingWorkerPool(mock_node_registry)
    
>       results = await worker_pool.process_chunks(mock_chunks)

tests/pipeline/test_processing_workers.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <src.pipeline.processing_workers.ProcessingWorkerPool object at 0x71ca5b0c2d30>
chunks = [namespace(chunk_id='test_chunk_0', data=b'test data content 0test data content 0test data content 0test data content ...t data content 5test data content 5test data content 5test data content 5test data content 5test data content 5'), ...]

    async def process_chunks(self, chunks: List) -> List[ProceessingFunction]:
        """main entry: here is whrere we will process all chunks
        across all the available nodes
        Args: chunks: list of datachunk objects from ingestion engine
        Returns:List of ProcessingTask results"""
    
        print(f"\n⚡ Starting distributed processing of {len(chunks)} chunks...")
        #initialisze node work load tracking
        self._initialize_node_workloads()
    
        #make procssing tasks form the chunks
        self.pending_tasks=[
            ProcessingTask(
                task_id=f"task_{i}",
                chunk_id=chunk.chunk_id,
                chunk_data=chunk.data
            )
            for i, chunk in enumerate(chunks) #not sure about htis for loop location
            #i get it but will future me get it/like it/swear  at me? yes
        ]
    
        await self._process_tasks_with_concurrency()
    
        #make summaryy
    
        totatotall_tasks = len(self.completed_tasks) + len(self.failed_tasks)
>       success_rate = len(self.completed_tasks) / total_tasks if total_tasks > 0 else 0
E       NameError: name 'total_tasks' is not defined

src/pipeline/processing_workers.py:203: NameError
------------------------------------------------------------ Captured stdout call ------------------------------------------------------------
⚡ Processing Worker Pool initialized
   Max workers per node: 4
   Load balancing: least_loaded
   Processing pipeline: 2 steps
   Simulation mode: True

⚡ Starting distributed processing of 10 chunks...
========================================================== short test summary info ===========================================================
FAILED tests/pipeline/test_processing_workers.py::test_process_chunks_success - NameError: name 'total_tasks' is not defined
======================================================== 1 failed, 1 passed in 0.30s =========================================================

```
typo in making totaltasks var

```
bash

(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 2 items                                                                                                                            

tests/pipeline/test_processing_workers.py ..                                                                                           [100%]

============================================================= 2 passed in 0.24s =============================================================



```
Finally


An Non-typo error:
```
bash

PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 5 items                                                                                                                            

tests/pipeline/test_processing_workers.py ....F                                                                                        [100%]

================================================================== FAILURES ==================================================================
___________________________________________________________ test_retry_on_failure ____________________________________________________________

mock_node_registry = namespace(nodes={'aws-node-1': namespace(node_id='aws-node-1', cloud_provider='aws', status='healthy'), 'gcp-node-1': ...vider='gcp', status='healthy'), 'gcp-node-2': namespace(node_id='gcp-node-2', cloud_provider='gcp', status='healthy')})

    @pytest.mark.asyncio
    async def test_retry_on_failure(mock_node_registry):
        """Test retry logic for failed processing"""
        worker_pool = ProcessingWorkerPool(mock_node_registry)
    
        # Create chunk that will fail first time
        failing_chunk = SimpleNamespace(
            chunk_id='failing_chunk',
            data=b''  # Empty data will fail validation
        )
    
        # Process with retries enabled
        results = await worker_pool.process_chunks([failing_chunk])
    
        # Should have attempted retries
        assert len(results) == 1
        task = results[0]
>       assert task.attempts > 1  # Should have retried
E       AssertionError: assert 0 > 1
E        +  where 0 = ProcessingTask(task_id='task_0', chunk_id='failing_chunk', chunk_data=b'', status=<ProcessingStatus.COMPLETED: 'comple...e='aws-node-1', attempts=0, start_time=1759432795.5740736, end_time=1759432795.6747441, error_message=None, result=b'').attempts

tests/pipeline/test_processing_workers.py:125: AssertionError
------------------------------------------------------------ Captured stdout call ------------------------------------------------------------
⚡ Processing Worker Pool initialized
   Max workers per node: 4
   Load balancing: least_loaded
   Processing pipeline: 2 steps
   Simulation mode: True

⚡ Starting distributed processing of 1 chunks...

✅ Processing complete:
   Total tasks: 1
   Completed: 1
   Failed: 0
   Success rate: 100.0%
========================================================== short test summary info ===========================================================
FAILED tests/pipeline/test_processing_workers.py::test_retry_on_failure - AssertionError: assert 0 > 1


```

Here we nedd to ammend this featrue to False to get it to work.
The test was because the failing_chunk (with empty data) is not actually failing validation as expected. The output  shows Completed: 1 and Success rate: 100.0%.

This is happening because the ProcessingWorkerPool is currently in simulation mode (self.simulate_processing = True). When simulate_processing is true, the _execute_processing_pipeline method simply returns the data unchanged and doesn't run the  actual processing functions (like DataValidator).

To properly test the retry logic, temporarily disable simulation mode within this test so that the DataValidator  actually runs and raises the ValueError for the empty chunk, and we adjust that in the function, not 'test-bed-global'.


```
bash
$ PYTHONPATH=. pytest tests/pipeline/test_processing_workers.py
============================================================ test session starts =============================================================
platform linux -- Python 3.9.23, pytest-8.4.2, pluggy-1.6.0
rootdir: /home/dell-linux-dev3/Projects/MultiCLoudTestsingSystem
plugins: asyncio-1.2.0, cov-7.0.0
asyncio: mode=strict, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collected 7 items                                                                                                                            

tests/pipeline/test_processing_workers.py ......F                                                                                      [100%]

================================================================== FAILURES ==================================================================
_________________________________________________________ test_processing_statistics _________________________________________________________

mock_node_registry = namespace(nodes={'aws-node-1': namespace(node_id='aws-node-1', cloud_provider='aws', status='healthy'), 'gcp-node-1': ...vider='gcp', status='healthy'), 'gcp-node-2': namespace(node_id='gcp-node-2', cloud_provider='gcp', status='healthy')})
mock_chunks = [namespace(chunk_id='test_chunk_0', data=b'test data content 0test data content 0test data content 0test data content ...t data content 5test data content 5test data content 5test data content 5test data content 5test data content 5'), ...]

    @pytest.mark.asyncio
    async def test_processing_statistics(mock_node_registry, mock_chunks):
        """Test processing statistics are accurate"""
        worker_pool = ProcessingWorkerPool(mock_node_registry)
    
        results = await worker_pool.process_chunks(mock_chunks)
>       stats = worker_pool.get_processing_statistics()

tests/pipeline/test_processing_workers.py:152: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/pipeline/processing_workers.py:377: in get_processing_statistics
    completed_durations = [t.duration_seconds() for t in self.completed_tasks]
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <list_iterator object at 0x716603dfea30>

>   completed_durations = [t.duration_seconds() for t in self.completed_tasks]
E   AttributeError: 'ProcessingTask' object has no attribute 'duration_seconds'

src/pipeline/processing_workers.py:377: AttributeError
------------------------------------------------------------ Captured stdout call ------------------------------------------------------------
⚡ Processing Worker Pool initialized
   Max workers per node: 4
   Load balancing: least_loaded
   Processing pipeline: 2 steps
   Simulation mode: True

⚡ Starting distributed processing of 10 chunks...

✅ Processing complete:
   Total tasks: 10
   Completed: 10
   Failed: 0
   Success rate: 100.0%
========================================================== short test summary info ===========================================================
FAILED tests/pipeline/test_processing_workers.py::test_processing_statistics - AttributeError: 'ProcessingTask' object has no attribute 'duration_seconds'
======================================================== 1 failed, 6 passed in 16.57s ========================================================
(multicloudtest) dell-linux-dev3@dell-linux-dev3-Precision-3591:~/Projects/MultiCLoudTestsingSystem$ @pytest.mark.asyncio
async def test_processing_statistics(mock_node_registry, mock_chunks):
    """Test processing statistics are accurate"""
    worker_pool = ProcessingWorkerPool(mock_node_registry)
    
    results = await worker_pool.process_chunks(mock_chunks)
    stats = worker_pool.get_processing_statistics()
    
    # Verify statistics
    assert stats['total_tasks'] == len(mock_chunks)
    assert stats['completed'] + stats['failed'] == len(mock_chunks)
    assert 0 <= stats['success_rate'] <= 1.0
    assert stats['average_duration_seconds'] >= 0@pytest.mark.asyncio
async def test_processing_statistics(mock_node_registry, mock_chunks):
    """Test processing statistics are accurate"""
    worker_pool = ProcessingWorkerPool(mock_node_registry)
    
    results = await worker_pool.process_chunks(mock_chunks)
    stats = worker_pool.get_processing_statistics()
    
    # Verify statistics
    assert stats['total_tasks'] == len(mock_chunks)
    assert stats['completed'] + stats['failed'] == len(mock_chunks)
    assert 0 <= stats['success_rate'] <= 1.0
    assert stats['average_duration_seconds'] >= 0^C
```

Here i tried to use an attribute i failed to put in the porcessingTask dataclass, so we add this to the dataclass:

``` 
python

def duration_seconds(self) -> float:
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        return 0.0

```