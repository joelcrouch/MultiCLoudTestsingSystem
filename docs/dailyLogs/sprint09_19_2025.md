# Daily Log: 2025-09-19 - Sprint 1: Foundation

## ‚úÖ Accomplishments

Today I successfully laid the complete software foundation for Sprint 1. I built and unit-tested all the core components required for a multi-cloud distributed system to begin operating.

1.  **Project Scaffolding**: Established a clean project structure with `src`, `tests`, and `docs` directories.
2.  **Environment Setup**: Created a `requirements.txt` file and established a `conda` environment, ensuring a reproducible setup.
3.  **TDD Workflow**: Implemented a Test-Driven Development workflow from the start, creating unit tests for every component.
4.  **Core Components Built & Tested**:
    *   `UnifiedAuthManager`: To handle multi-cloud authentication.
    *   `MultiCloudNodeRegistry`: To track nodes and their health with an adaptive timeout mechanism.
    *   `CrossCloudCommunicationProtocol`: For inter-node messaging and failure handling.
    *   `ConfigurationManager`: To load and validate cluster settings from YAML files.
5.  **Enhanced Testing Suite**: Integrated the `green` test runner for a more readable, color-coded, and efficient testing experience that can discover and run all tests project-wide.
6.  **Version Control**: Initialized a Git repository, committed the foundational code to the `main` branch, and created a `feature/S1-getRealNodesUp` branch for our next deployment tasks.
7.  **CLI Tooling**: Installed the AWS and GCP command-line interfaces, preparing the local machine for cloud infrastructure provisioning.

## ‚ùó Errors & Resolutions

I encountered and resolved several issues, which are crucial for our research documentation:

1.  **`SyntaxError` in `cloud_auth_manager.py`**: The initial file contained syntax errors. **Solution**: Corrected the file by overwriting it with the valid code from our plan.
2.  **`ModuleNotFoundError: No module named 'boto3'`**: The tests are failing because they are using the system's Python interpreter instead of the project's `conda` environment. **Solution**: Ran the tests using `python3 -m unittest ...` to ensure the correct interpreter was used.
3.  **`RuntimeWarning: coroutine was never awaited`**: The async tests for the communication protocol were not structured correctly for the `unittest` runner. **Solution**: Restructured the test file to use `asyncio.run()` within synchronous test methods, which resolved the warnings.
4.  **`AttributeError` in `ConfigurationManager`**: The `failure_log` attribute was being initialized *after* the method that could fail, causing an error on configuration failure. **Solution**: Reordered the lines in the `__init__` method to initialize `failure_log` before any fallible operation.
5.  **`sudo: a terminal is required`**: The automated installation of the AWS CLI failed because `sudo` required an interactive password prompt. **Solution**: I run the `sudo ./aws/install` command manually in the terminal.

## üìä Current Status

I have successfully completed the **software implementation phase of Sprint 1**. All foundational components are coded, unit-tested, and committed to the `main` branch. The project is now on the `feature/S1-getRealNodesUp` branch, ready for the deployment and integration phase. The AWS and GCP CLIs are installed, pending your configuration.

---

## üöÄ Next Step

1.  You will configure the AWS and GCP CLIs with your credentials by running `aws configure` and `gcloud init` in your terminal.
2.  Once that is done, I will provide the scripts to provision the 6-node cluster across AWS and GCP.

---

## üìä Update: Cloud Environment Setup

I have successfully configured the local development environment to communicate with both AWS and GCP.

### ‚úÖ Accomplishments

1.  **AWS CLI Configuration**:
    *   Guided the creation of a new IAM user and group with a least-privilege JSON policy to ensure secure access.
    *   Configured the AWS CLI with the new user's credentials, a default region (`us-east-1`), and output format (`json`).

2.  **GCP CLI Configuration**:
    *   Guided the installation of the `gcloud` CLI, including resolving a `snap` classic confinement issue.
    *   Walked through the `gcloud init` process, including creating a new GCP project.
    *   Enabled the **Compute Engine API**, a necessary prerequisite for creating VMs.
    *   Set the default compute region (`us-central1`) and zone (`us-central1-a`).

### üöÄ Next Step

The local environment is now fully prepared for cloud resource provisioning.

1.  I am waiting for you to provide two pieces of information:
    *   The name of an **EC2 Key Pair** in the `us-east-1` region.
    *   The **HTTPS URL** of the project's Git repository.
2.  Once I have this information, I will generate the `deployment/provision_aws.sh` and `deployment/provision_gcp.sh` scripts to automate the creation of the 6-node cluster.

---

## ‚òÅÔ∏è Cloud Environment Setup Summary (Sprint 1)

This section details the steps taken to configure the local environment for multi-cloud operations and prepare for cluster provisioning.

### AWS CLI Configuration

1.  **IAM User & Policy Creation**:
    *   A new IAM user was created in the AWS console.
    *   A custom IAM policy (`MultiCloudExperimentPolicy`) was defined with least-privilege permissions for EC2 instance management (create, describe, terminate) and security group management.
    *   The policy was attached to a new IAM group, and the user was added to this group.
    *   An Access Key ID and Secret Access Key were generated for the user, with the use case specified as "Command Line Interface (CLI)".
    *   **Policy JSON:**
        ```json
        {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Effect": "Allow",
                    "Action": [
                        "ec2:DescribeInstances",
                        "ec2:DescribeImages",
                        "ec2:DescribeKeyPairs",
                        "ec2:DescribeSecurityGroups",
                        "ec2:DescribeSubnets",
                        "ec2:DescribeVpcs",
                        "ec2:CreateSecurityGroup",
                        "ec2:AuthorizeSecurityGroupIngress",
                        "ec2:DeleteSecurityGroup",
                        "ec2:RunInstances",
                        "ec2:TerminateInstances",
                        "ec2:CreateTags"
                    ],
                    "Resource": "*"
                }
            ]
        }
        ```

2.  **`aws configure` Execution**:
    *   The `aws configure` command was run locally.
    *   **Access Key ID:** Provided.
    *   **Secret Access Key:** Provided.
    *   **Default region:** `us-east-1` (US East - N. Virginia) was selected as a cost-effective region.
    *   **Default output format:** `json` was selected for scripting compatibility.

3.  **EC2 Key Pair**:
    *   An EC2 Key Pair named `multicloudKey2_east_1` was created in the `us-east-1` region for SSH access to instances.

4.  **Current Status**: AWS account is undergoing `PendingVerification` for resource launching in `us-east-1`.

### GCP CLI Configuration

1.  **`gcloud` CLI Installation**:
    *   The `google-cloud-cli` was installed via `snap`.
    *   A `snap` classic confinement warning was resolved by using `sudo snap install google-cloud-cli --classic`.

2.  **`gcloud init` Execution**:
    *   The `gcloud init` command was run locally.
    *   User authenticated with their Google account (`joelcrouch@gmail.com`).
    *   A new GCP project was created via the web UI (after an initial CLI failure) and selected: `norse-ward-472620-r0`.
    *   The "getting started" survey was answered with "I'm not sure or I don't see my answer".
    *   The `gcloud` configuration was re-initialized to associate with the new project.

3.  **Compute Engine API & Region/Zone Setup**:
    *   The `compute.googleapis.com` API was enabled for the project using `gcloud services enable compute.googleapis.com` (after correcting a typo).
    *   Default compute region and zone were set:
        *   `gcloud config set compute/region us-central1`
        *   `gcloud config set compute/zone us-central1-a`

4.  **GCP Instance Provisioning**:
    *   The `deployment/provision_gcp.sh` script was executed successfully, launching 3 `e2-micro` preemptible instances in `us-central1-a`.
    *   The script also created a firewall rule `multi-cloud-firewall` allowing SSH and app traffic on port `8080`.
    *   Verification confirmed the repository was cloned to `/home/multiCloudDistSys` and owned by `root`, requiring `sudo chown` for user access.

5.  **GCP Instance Teardown**:
    *   The `deployment/teardown_gcp.sh` script was executed to delete the GCP instances and firewall rule. The user opted to keep the interactive confirmation for deletion.

### Provisioning & Teardown Scripts

The following scripts were created in the `deployment/` directory:

*   `provision_aws.sh`: Automates the creation of 3 AWS EC2 instances.
*   `provision_gcp.sh`: Automates the creation of 3 GCP Compute Engine instances.
*   `teardown_aws.sh`: Automates the termination of AWS EC2 instances and deletion of security groups.
*   `teardown_gcp.sh`: Automates the deletion of GCP Compute Engine instances and firewall rules.

### Current Overall Status

*   AWS provisioning is on hold pending account verification.
*   GCP instances have been successfully provisioned and then torn down.
*   The local environment is fully configured for both clouds.

### Next Steps

1.  Wait for AWS account verification to complete.
2.  Once AWS is verified, re-run `./deployment/provision_aws.sh`.
3.  Re-run `./deployment/provision_gcp.sh` to bring GCP instances back online.
4.  Verify inter-cloud communication between the 6 nodes.
