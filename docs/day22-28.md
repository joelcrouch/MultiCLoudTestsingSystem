make the orchestrator
```
src/pipleline/pipeline_orchestrator.py
# src/pipeline/pipeline_orchestrator.py
class PipelineOrchestrator:
    """
    Coordinates complete end-to-end pipeline execution
    """
    
    def __init__(self, node_registry, config_dir='config/'):
        self.node_registry = node_registry
        
        # Initialize all pipeline stages
        self.ingestion_engine = DataIngestionEngine(node_registry)
        self.processing_pool = ProcessingWorkerPool(node_registry)
        self.distribution_coordinator = DistributionCoordinator(node_registry)
        self.storage_manager = StorageManager(node_registry)
        
        # Pipeline state tracking
        self.pipeline_status = PipelineStatus.IDLE
        self.current_batch = None
        self.metrics = PipelineMetrics()
    
    async def run_pipeline(self, batch_config: Dict) -> PipelineResult:
        """
        Execute complete pipeline: Ingestion -> Processing -> Distribution -> Storage
        """
        print(f"\n{'='*60}")
        print(f"Starting Pipeline Execution")
        print(f"Batch: {batch_config['batch_id']}")
        print(f"{'='*60}\n")
        
        start_time = time.time()
        self.pipeline_status = PipelineStatus.RUNNING
        
        try:
            # Stage 1: Data Ingestion
            print("STAGE 1: Data Ingestion")
            ingested_chunks = await self.ingestion_engine.ingest_batch(
                batch_config['data_source']
            )
            self.metrics.record_stage('ingestion', len(ingested_chunks))
            
            # Stage 2: Processing
            print("\nSTAGE 2: Processing")
            processed_chunks = await self.processing_pool.process_chunks(
                ingested_chunks
            )
            self.metrics.record_stage('processing', len(processed_chunks))
            
            # Stage 3: Distribution
            print("\nSTAGE 3: Distribution")
            distributed_chunks = await self.distribution_coordinator.distribute_processed_chunks(
                processed_chunks
            )
            self.metrics.record_stage('distribution', len(distributed_chunks))
            
            # Stage 4: Storage
            print("\nSTAGE 4: Storage")
            stored_chunks = await self.storage_manager.store_distributed_chunks(
                distributed_chunks
            )
            self.metrics.record_stage('storage', len(stored_chunks))
            
            # Calculate final metrics
            duration = time.time() - start_time
            self.pipeline_status = PipelineStatus.COMPLETED
            
            return PipelineResult(
                status='success',
                duration_seconds=duration,
                chunks_processed=len(ingested_chunks),
                metrics=self.metrics.get_summary()
            )
            
        except Exception as e:
            self.pipeline_status = PipelineStatus.FAILED
            print(f"\n❌ Pipeline failed: {e}")
            return PipelineResult(
                status='failed',
                error=str(e),
                metrics=self.metrics.get_summary()
            )

```

meak an ntigration test:

```# tests/integration/test_full_pipeline.py
@pytest.mark.asyncio
async def test_complete_pipeline_execution(setup_test_cluster):
    """Test complete end-to-end pipeline"""
    
    # Setup
    node_registry = setup_test_cluster
    orchestrator = PipelineOrchestrator(node_registry)
    
    # Create test batch config
    batch_config = {
        'batch_id': 'test_batch_001',
        'data_source': './test_data/gcp_gcs_simulation',
        'expected_size_mb': 500
    }
    
    # Run pipeline
    result = await orchestrator.run_pipeline(batch_config)
    
    # Verify success
    assert result.status == 'success'
    assert result.chunks_processed > 0
    assert result.duration_seconds < 300  # Should complete in < 5 minutes
    
    # Verify all stages completed
    assert result.metrics['ingestion']['success_rate'] > 0.95
    assert result.metrics['processing']['success_rate'] > 0.95
    assert result.metrics['distribution']['replica_success_rate'] > 0.99
    assert result.metrics['storage']['success_rate'] > 0.95
    ```
    not so sure aout these numbrers above


    cross clodi integration aws-gcp works with data and as a system

    cross clodu flwo tests  

    ```
    # tests/integration/test_cross_cloud_flow.py
@pytest.mark.asyncio
async def test_data_flows_between_clouds():
    """Verify data successfully transfers between AWS and GCP nodes"""
    
    # Setup nodes in different clouds
    node_registry = create_multi_cloud_registry()
    orchestrator = PipelineOrchestrator(node_registry)
    
    # Configure to force cross-cloud transfers
    test_config = {
        'batch_id': 'cross_cloud_test',
        'data_source': './test_data/aws_s3_simulation',  # Start in AWS
        'force_cross_cloud': True
    }
    
    result = await orchestrator.run_pipeline(test_config)
    
    # Verify data reached GCP nodes
    storage_stats = result.metrics['storage']['by_cloud']
    assert 'gcp' in storage_stats
    assert storage_stats['gcp']['chunk_count'] > 0
    
    # Verify cross-cloud replicas exist
    dist_stats = result.metrics['distribution']
    assert dist_stats['cross_cloud_transfers'] > 0
    ```


    Network failure sim
    Deliverable: Verified cross-cloud functionality with basic failure handling
    ```
    @pytest.mark.asyncio
async def test_pipeline_continues_with_node_failure():
    """Test pipeline handles node failure gracefully"""
    
    orchestrator = PipelineOrchestrator(node_registry)
    
    # Start pipeline
    pipeline_task = asyncio.create_task(
        orchestrator.run_pipeline(test_config)
    )
    
    # Wait for 50% completion
    await asyncio.sleep(2)
    
    # Kill one node
    node_registry.nodes['aws-node-2'].status = 'unhealthy'
    
    # Pipeline should complete despite failure
    result = await pipeline_task
    
    assert result.status == 'success'
    assert 'node_failures_handled' in result.metrics
    ```


    tests coverage pusshhhhhhhhhh

    int tests:
    ```
    # tests/unit/test_ingestion_chunking.py
def test_chunk_file_splits_correctly():
    """Test file chunking logic"""
    engine = DataIngestionEngine(mock_registry)
    
    # 250MB file with 100MB chunks should produce 3 chunks
    test_data = b'x' * (250 * 1024 * 1024)
    chunks = await engine.chunk_file('test.dat', test_data)
    
    assert len(chunks) == 3
    assert chunks[0].size_bytes == 100 * 1024 * 1024
    assert chunks[2].size_bytes == 50 * 1024 * 1024

# tests/unit/test_placement_strategy.py
def test_network_aware_placement_prefers_same_cloud():
    """Test placement algorithm"""
    placement = NetworkAwarePlacement(mock_registry, network_topology, config)
    
    selected = placement.select_target_nodes(
        chunk_id='test',
        source_node='aws-node-1',
        num_replicas=3
    )
    
    # Should prefer AWS nodes when source is AWS
    aws_count = sum(1 for node in selected if 'aws' in node)
    assert aws_count >= 2
    ```
    integrations tesst

    ```
    # tests/integration/test_stage_integration.py
@pytest.mark.asyncio
async def test_ingestion_to_processing_handoff():
    """Test data flows correctly between stages"""
    ingestion = DataIngestionEngine(node_registry)
    processing = ProcessingWorkerPool(node_registry)
    
    # Ingest data
    chunks = await ingestion.ingest_batch(test_config)
    
    # Process ingested chunks
    results = await processing.process_chunks(chunks)
    
    # Verify no data loss
    assert len(results) == len(chunks)
    assert all(r.result is not None for r in results)
    ```

    performance tests

    ```
    # tests/performance/test_throughput.py
@pytest.mark.asyncio
async def test_processing_latency_under_100ms():
    """Verify processing meets latency requirements"""
    worker_pool = ProcessingWorkerPool(node_registry)
    
    chunks = create_test_chunks(count=100, size_kb=100)

    start = time.time()
    results = await worker_pool.process_chunks(chunks)
    duration = time.time() - start
    
    avg_latency = duration / len(chunks)
    assert avg_latency < 0.1  # <100ms average

@pytest.mark.asyncio
async def test_ingestion_throughput():
    """Test ingestion can handle target throughput"""
    engine = DataIngestionEngine(node_registry)
    
    # Test with 1GB file
    start = time.time()
    chunks = await engine.ingest_batch('1gb_test_file.dat')
    duration = time.time() - start
    
    throughput_mbps = (1024 / duration)  # MB/s
    assert throughput_mbps > 200  # >200 MB/s target
    ```

    f
    profileing and poptimizing:
    ```
    # src/monitoring/performance_profiler.py
class PerformanceProfiler:
    """Profile pipeline performance to identify bottlenecks"""
    
    def __init__(self):
        self.stage_times = defaultdict(list)
        self.bottlenecks = []
    
    def profile_pipeline_run(self, orchestrator):
        """Profile a complete pipeline run"""
        
        # Collect timing for each stage
        stage_metrics = orchestrator.metrics.get_stage_breakdown()
        
        # Identify bottlenecks (stages taking >30% of total time)
        total_time = sum(stage_metrics.values())
        
        for stage, duration in stage_metrics.items():
            percentage = (duration / total_time) * 100
            if percentage > 30:
                self.bottlenecks.append({
                    'stage': stage,
                    'duration': duration,
                    'percentage': percentage
                })
        
        return self.generate_report()

    ```

    runem:
    ```
    # scripts/profile_pipeline.py
async def main():
    profiler = PerformanceProfiler()
    orchestrator = PipelineOrchestrator(node_registry)
    
    # Run multiple iterations
    for i in range(5):
        print(f"Profiling run {i+1}/5...")
        result = await orchestrator.run_pipeline(test_config)
        profiler.record_run(result)
    
    # Generate analysis
    report = profiler.generate_report()
    print(report)
    
    # Output example:
    # Performance Analysis (5 runs):
    # 
    # Stage Breakdown:
    #   Ingestion:    12% (2.1s avg)
    #   Processing:   45% (8.2s avg) ⚠️ BOTTLENECK
    #   Distribution: 28% (5.1s avg)
    #   Storage:      15% (2.7s avg)
    #
    # Recommendations:
    #   1. Increase processing worker concurrency
    #   2. Consider async processing improvements
    #   3. Profile individual processing functions

    ```    

    basic optimiziations;
    ```
    # If processing is bottleneck, increase concurrency
processing_config.yml:
  max_workers_per_node: 4 -> 8  # Double workers

# If distribution is bottleneck, increase concurrent transfers
distribution_config.yml:
  max_concurrent_distributions: 15 -> 25

# If network is bottleneck, optimize chunk size
ingestion_config.yml:
  chunk_size_mb: 100 -> 50  # Smaller chunks = more parallelism

    ```    

    monitoring and observabilitys 

    ```
    # src/monitoring/status_dashboard.py
class StatusDashboard:
    """Simple terminal-based status dashboard"""
    
    def display_pipeline_status(self, orchestrator):
        """Show real-time pipeline status"""
        
        status = f"""
        ╔════════════════════════════════════════╗
        ║     Pipeline Status Dashboard          ║
        ╠════════════════════════════════════════╣
        ║ Status: {orchestrator.pipeline_status}
        ║ Current Stage: {orchestrator.current_stage}
        ║ Progress: {orchestrator.progress_percent}%
        ║
        ║ Nodes:
        ║   Healthy: {orchestrator.healthy_nodes}
        ║   Unhealthy: {orchestrator.unhealthy_nodes}
        ║
        ║ Throughput: {orchestrator.current_throughput} MB/s
        ║ ETA: {orchestrator.estimated_completion}
        ╚════════════════════════════════════════╝
        """
        print(status)

    ```


    more structuere logging

    ```
    # src/monitoring/pipeline_logger.py
import logging
import json

class PipelineLogger:
    """Structured logging for pipeline events"""
    
    def __init__(self):
        self.logger = logging.getLogger('pipeline')
        handler = logging.FileHandler('logs/pipeline.log')
        handler.setFormatter(logging.Formatter('%(message)s'))
        self.logger.addHandler(handler)
    
    def log_stage_complete(self, stage, metrics):
        """Log stage completion with metrics"""
        self.logger.info(json.dumps({
            'event': 'stage_complete',
            'stage': stage,
            'timestamp': time.time(),
            'metrics': metrics
        }))


    ```

    documanttion and sprint wrap 


    ```

    # docs/architecture.md

## System Architecture

### Overview
Multi-cloud distributed data pipeline with 4 stages:
- Ingestion: Pull data from sources
- Processing: Transform and validate
- Distribution: Replicate across nodes
- Storage: Persist with integrity checks

### Component Diagram
[Include diagram showing nodes, clouds, data flow]

### Key Design Decisions
1. 3x replication for fault tolerance
2. Network-aware placement for performance
3. Quorum writes (2/3 minimum)
4. Adaptive timeouts for cross-cloud latency


setup instructoins
# docs/setup.md

## Quick Start

### Prerequisites
- Python 3.9+
- 2GB free disk space
- Multiple terminal windows for multi-node simulation

### Installation
```bash
git clone https://github.com/yourusername/multi-cloud-orchestrator
cd multi-cloud-orchestrator
pip install -r requirements.txt

run a demo
# Terminal 1: Start node registry
python src/nodes/node_registry.py

# Terminal 2-4: Start worker nodes
CLOUD_PROVIDER=aws python src/nodes/worker_node.py
CLOUD_PROVIDER=gcp python src/nodes/worker_node.py
CLOUD_PROVIDER=aws python src/nodes/worker_node.py

# Terminal 5: Run pipeline
python scripts/run_demo_pipeline.py


    ```


    demo script 

```
**Demo Script:**
```python
# scripts/run_demo_pipeline.py
"""
5-minute demo script for interviews
"""

async def run_demo():
    print("Multi-Cloud Data Pipeline Orchestrator Demo")
    print("=" * 60)
    
    # Show cluster status
    print("\n1. Cluster Status:")
    show_cluster_status(node_registry)
    
    # Run pipeline
    print("\n2. Running Pipeline (1GB test dataset):")
    result = await orchestrator.run_pipeline(demo_config)
    
    # Show results
    print("\n3. Results:")
    display_results(result)
    
    # Demonstrate failure recovery
    print("\n4. Failure Recovery Demo:")
    await demonstrate_node_failure(orchestrator)
    
    # Show final stats
    print("\n5. Final Statistics:")
    display_statistics(orchestrator)

if __name__ == '__main__':
    asyncio.run(run_demo())


```
    